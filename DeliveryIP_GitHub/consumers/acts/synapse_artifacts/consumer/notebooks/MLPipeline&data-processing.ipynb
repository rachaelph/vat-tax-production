{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Data processing notebook and scripts to read data from ADLS and perform processing",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data Processing\r\n",
        "This notebook main task is to perform data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql.functions import isnan, when, count, col\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "from azure.storage.blob import ContainerClient, BlobClient, BlobServiceClient\r\n",
        "from io import BytesIO, StringIO\r\n",
        "from datetime import datetime, timedelta\r\n",
        "from notebookutils import mssparkutils\r\n",
        "\r\n",
        "# set the notebook completed flag\r\n",
        "notebook_completed_status = 'not_completed'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Import common constants and variables\r\n",
        "Importing constants from a notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run common/constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data Ingestion\r\n",
        "Ingesting the VAT Txx dataset from Azure Data Lake Store (ADLS)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# read the specific fields from ADLS\r\n",
        "try:\r\n",
        "\r\n",
        "    # read the current vat tax dataframe based on the most updtaed day\r\n",
        "    # setup the main ADLS connection string\r\n",
        "    PATH = f'abfss://{STAGING_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{VAT_TAX_FOLDER}/'\r\n",
        "    \r\n",
        "    # get the latest day loaded into ADLS\r\n",
        "    vat_tax_files = mssparkutils.fs.ls(PATH)\r\n",
        "    dates_folder = []\r\n",
        "\r\n",
        "    for file in vat_tax_files:\r\n",
        "        dates_folder.append(datetime.strptime(file.name, '%Y-%m-%d'))\r\n",
        "    if len(dates_folder) < 1:\r\n",
        "        raise Exception(f'{PATH} has no date (day) refrenced')    \r\n",
        "    \r\n",
        "    # get the most current date \r\n",
        "    current_date = max(dates_folder).strftime('%Y-%-m-%-d')\r\n",
        "\r\n",
        "    # now setup the connection string with current date\r\n",
        "    CONNECTION_STR = f'abfss://{STAGING_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{VAT_TAX_FOLDER}/{current_date}/'\r\n",
        "\r\n",
        "    tax_data = spark.read.load(path=CONNECTION_STR, format='parquet', header=True)\r\n",
        "    dataframe_records = tax_data.count()\r\n",
        "\r\n",
        "    # perform a empty dataframe test and throw exception if required\r\n",
        "    if dataframe_records <= RECORDS_THRESH:\r\n",
        "        raise Exception('Dataframe has low records count')\r\n",
        "\r\n",
        "    tax_data.show(10)\r\n",
        "    print('Raw current tax datafrem rows: ', dataframe_records, 'and columns: ', len(tax_data.columns))\r\n",
        "\r\n",
        "except Exception as error:\r\n",
        "    print(f'Error in {error}')\r\n",
        "    raise ValueError(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "tax_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data pre-processing\r\n",
        "Perform data pre-processing including, type casting, remove nulls, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "try:\r\n",
        "    # 1. drop duplicate rows across all columns\r\n",
        "    tax_data = tax_data.drop_duplicates()\r\n",
        "    # save the duplate rows data quality also\r\n",
        "    duplicates_ratio = tax_data.distinct().count() / tax_data.count()\r\n",
        "    print(f'Dataframe completenss ratio (duplicates) is {duplicates_ratio}')\r\n",
        "\r\n",
        "    # 2. Calculate null values for each column\r\n",
        "    df_null_ratio = tax_data.agg(*[(1-F.count(c) / F.count('*')).alias(c) for c in tax_data.columns]).toPandas()\r\n",
        "    drop_columns = list(df_null_ratio.loc[:, (df_null_ratio >= COLUMN_QUALITY_THRESH).any()].columns)\r\n",
        "\r\n",
        "    print(f'{len(drop_columns)} columns will be dropped for null test.')\r\n",
        "    tax_data = tax_data.drop(*drop_columns)\r\n",
        "    # save the parameter for MLOps data quality trigger\r\n",
        "    data_quality_nulls = len(drop_columns) / len(tax_data.columns)\r\n",
        "\r\n",
        "    # 3. drop columns for low variances\r\n",
        "    df_low_variance = tax_data.agg(*[F.count_distinct(c).alias(c).alias(c) for c in tax_data.columns]).toPandas()\r\n",
        "    drop_columns = list(df_low_variance.loc[:, (df_low_variance <= COLUMN_VARIANCE_THRESH).any()].columns)\r\n",
        "\r\n",
        "    print(f'{len(drop_columns)} columns will be dropped for low variance test.')\r\n",
        "    tax_data = tax_data.drop(*drop_columns)\r\n",
        "\r\n",
        "    # save the parameter for MLOps data quality trigger\r\n",
        "    data_quality_variance = len(drop_columns) / len(tax_data.columns)\r\n",
        "\r\n",
        "    print(f'New dataframe row: {tax_data.count()}, columns: {len(tax_data.columns)}')\r\n",
        "\r\n",
        "except Exception as error:\r\n",
        "    print(f'Error in {error}')   \r\n",
        "    raise ValueError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data Quality Analysis Output\r\n",
        "Performing data quality analysis and exporting the results to the pipeline for MLOps trigger and pipeline conditional triggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "try:\r\n",
        "    # set the data quality pass flag\r\n",
        "    data_qualtity = 'data_quality_passed'\r\n",
        "\r\n",
        "    # perfrom a basic data quality test to determine if exception should be thrown\r\n",
        "    if ((data_quality_nulls >= COLUMN_QUALITY_THRESH) or (data_quality_variance >= COLUMN_VARIANCE_THRESH)):\r\n",
        "        data_qualtity = 'data_quality_failed'\r\n",
        "        raise Exception(f'The dataframe did not pass data quality check')\r\n",
        "    else:\r\n",
        "        data_qualtity = 'data_quality_passed'\r\n",
        "        \r\n",
        "except Exception as error:\r\n",
        "    print(f'Error in {error}')\r\n",
        "    raise ValueError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write to Datastore\r\n",
        "Writing results to the processed Azure Datalake store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "try:\r\n",
        "    # Instantiate a BlobServiceClient using a connection string\r\n",
        "    CONNECTION_STR = f'abfss://{DATA_PROCESSED_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{VAT_TAX_PROCESSED_FOLDER}/{current_date}' \r\n",
        "   \r\n",
        "    # prepare to write\r\n",
        "    tax_data.write.format('csv').option('header', True).option('encoding', 'utf-8').mode('overwrite').save(CONNECTION_STR)\r\n",
        "\r\n",
        "except Exception as error:\r\n",
        "    print(f'Error in {error}') \r\n",
        "    raise ValueError(error)\r\n",
        "\r\n",
        "# this line has be be outside the try except block\r\n",
        "# exit the notebook with the data_quality value\r\n",
        "mssparkutils.notebook.exit(data_qualtity)"
      ]
    }
  ]
}