{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Notebook to analyse datadrift based on two dataframes",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data Drift Analyser\r\n",
        "This notebook is used to perfrom data drifts based on given dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "data_quality = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql.functions import isnan, when, count, col\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "from azure.storage.blob import ContainerClient, BlobClient, BlobServiceClient\r\n",
        "from io import BytesIO, StringIO\r\n",
        "from datetime import datetime, timedelta\r\n",
        "from ast import literal_eval\r\n",
        "from notebookutils import mssparkutils\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "\r\n",
        "# data drift\r\n",
        "from alibi_detect.cd import KSDrift\r\n",
        "\r\n",
        "# set the notebook completed flag\r\n",
        "notebook_completed_status = 'not_completed'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Import common constants and variables\r\n",
        "Importing constants from a notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run /common/constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data Ingestion\r\n",
        "Ingesting 2 sets of dataets from the ADLS containers:\r\n",
        "1. The current VAT TAx daily dataset\r\n",
        "2. The previous (baseline) VAT tax dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# read the specific fields from ADLS\r\n",
        "try:\r\n",
        "    # read the current vat tax dataframe based on the most updtaed day\r\n",
        "    # setup the main ADLS connection string\r\n",
        "    PATH = f'abfss://{STAGING_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{VAT_TAX_FOLDER}/'\r\n",
        "    \r\n",
        "    # get the latest day loaded into ADLS\r\n",
        "    vat_tax_files = mssparkutils.fs.ls(PATH)\r\n",
        "    dates_folder = []\r\n",
        "\r\n",
        "    for file in vat_tax_files:\r\n",
        "        dates_folder.append(datetime.strptime(file.name, '%Y-%m-%d'))\r\n",
        "    if len(dates_folder) < 1:\r\n",
        "        raise Exception(f'{PATH} has no date (day) refrenced')    \r\n",
        "    \r\n",
        "    # get the most current date and previous day \r\n",
        "    current_date = max(dates_folder).strftime('%Y-%-m-%-d')\r\n",
        "    previous_date = (max(dates_folder) - timedelta(days= 1)).strftime('%Y-%-m-%-d')\r\n",
        "\r\n",
        "    # now setup the connection string with current date\r\n",
        "    CONNECTION_STR_CURRENT_DAY = f'abfss://{STAGING_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{VAT_TAX_FOLDER}/{current_date}/'\r\n",
        "    # read the datasets\r\n",
        "    tax_data = spark.read.load(path=CONNECTION_STR_CURRENT_DAY, format='parquet', header=True)\r\n",
        "    dataframe_records = tax_data.count()\r\n",
        "\r\n",
        "    # read the previous day\r\n",
        "    CONNECTION_STR_PREVIOUS_DAY = f'abfss://{STAGING_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{VAT_TAX_FOLDER}/{previous_date}/'\r\n",
        "    tax_data_baseliine = spark.read.load(path=CONNECTION_STR_PREVIOUS_DAY, format='parquet', header=True)\r\n",
        "    dataframe_baseline_records = tax_data_baseliine.count()\r\n",
        "\r\n",
        "    tax_data.show(10)\r\n",
        "    print('Raw current tax datafrem rows: ', dataframe_records, 'and columns: ', len(tax_data.columns))\r\n",
        "    print('Raw baseline tax datafrem rows: ', dataframe_baseline_records, 'and columns: ', len(tax_data_baseliine.columns))\r\n",
        "\r\n",
        "except Exception as error:\r\n",
        "    print(f'Error in {error}')\r\n",
        "    raise ValueError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data Drift Analysis\r\n",
        "Performing statstical-based data drift, comparing current dataset with previous day (baseline) dataset\r\n",
        "The Kolmogorov-Smirnov (K-S) test method is implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "try:\r\n",
        "    def rank_feature_drift(preds, feature_names, p_val=0.05):\r\n",
        "        \"\"\" Rank likely drift contribution by feature.\r\n",
        "        \"\"\"\r\n",
        "        try:\r\n",
        "            drift_by_feature = pd.DataFrame()\r\n",
        "            \r\n",
        "            vals = preds[\"data\"][\"p_val\"]\r\n",
        "\r\n",
        "            # First check the number of features and prediction p-values match\r\n",
        "            try:\r\n",
        "                assert len(feature_names) == len(vals)\r\n",
        "            except AssertionError:\r\n",
        "                print(\"Ensure prediction is being run with all features.\")\r\n",
        "\r\n",
        "            # Sort from lowest to highest p-value\r\n",
        "            # Lowest p-value indicates greatest confidence in distribution difference\r\n",
        "            sort_index = np.argsort(vals)  # argsort is in ascending order by default\r\n",
        "            features_sorted = [feature_names[idx] for idx in sort_index]\r\n",
        "            vals_sorted = vals[sort_index]\r\n",
        "\r\n",
        "            # Drift by feature\r\n",
        "            drift_by_feature = pd.DataFrame(\r\n",
        "                dict(\r\n",
        "                    {\r\n",
        "                        \"feature\": features_sorted,\r\n",
        "                        \"p_val\": vals_sorted,\r\n",
        "                        \"is_significant_drift\": vals_sorted < p_val,\r\n",
        "                    }\r\n",
        "                )\r\n",
        "            )\r\n",
        "\r\n",
        "        except Exception as error:\r\n",
        "            print(f'Error in {error}')\r\n",
        "            raise ValueError(error)\r\n",
        "\r\n",
        "        finally:\r\n",
        "            return drift_by_feature\r\n",
        "\r\n",
        "    # convert the spark dataframe to pandas (both the current and the previous)\r\n",
        "    tax_data_target_df = tax_data.toPandas()\r\n",
        "    tax_data_baseliine_df = tax_data_baseliine.toPandas() \r\n",
        "\r\n",
        "    # select which features to perform the K-S test\r\n",
        "    drift_feature_list = ['IncomeTax','IncomeTaxTarget','ValueAddedTax','TargetValueAddedTax','CorporationTax','TargetCorporationTax']\r\n",
        "\r\n",
        "    # setup the feature for drift analysis\r\n",
        "    feature_target = tax_data_target_df[drift_feature_list].dropna().to_numpy()\r\n",
        "    feature_baseline = tax_data_baseliine_df[drift_feature_list].dropna().to_numpy()\r\n",
        "\r\n",
        "    # Initialise the drift detector using the K-S method\r\n",
        "    KS_drift_model = KSDrift(x_ref=feature_baseline, p_val=0.05, alternative='two-sided')\r\n",
        "\r\n",
        "    # Perform the drift by feature_baseline\r\n",
        "    KS_drift_predict = KS_drift_model.predict(feature_target, return_p_val=True,return_distance=True)\r\n",
        "    KS_drift_by_feature = rank_feature_drift(KS_drift_predict, drift_feature_list)\r\n",
        "    KS_drift_df = pd.DataFrame(list(zip(drift_feature_list, KS_drift_predict.get('data').get('distance'), KS_drift_predict.get('data').get('p_val'))), \r\n",
        "    columns=['feature', 'drift_score', 'p_value']).sort_values(by='drift_score',ascending=False)\r\n",
        "\r\n",
        "    # show the results and plot\r\n",
        "    print(KS_drift_df)\r\n",
        "    ax = KS_drift_df.plot(x='feature', y='drift_score', kind='bar', figsize=(12,8), fontsize=12, legend=False)\r\n",
        "    ax.set_title('Feature drift scores', fontsize=16)\r\n",
        "\r\n",
        "    # plot the results\r\n",
        "except Exception as error:\r\n",
        "    print(f'Error in {error}')\r\n",
        "    raise ValueError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Prepare the drift resutls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# prepare the results of the drift analysis\r\n",
        "try:\r\n",
        "    # prepare the train or inference flag\r\n",
        "    train_or_inference = 'train'\r\n",
        "\r\n",
        "    # get the overall drift (its the average of the drifts for each feature\r\n",
        "    overall_drift = True if KS_drift_predict.get('data').get('is_drift') == '1' else False\r\n",
        "\r\n",
        "    # decide for train or inference based on drift\r\n",
        "    train_or_inference = 'train' if overall_drift else 'inference'\r\n",
        "\r\n",
        "except Exception as error:\r\n",
        "    print(f'Error in {error}')\r\n",
        "    raise ValueError(error)\r\n",
        "\r\n",
        "# this line has be be outside the try except block\r\n",
        "# exit the notebook with the data_quality value\r\n",
        "mssparkutils.notebook.exit(train_or_inference)"
      ]
    }
  ]
}