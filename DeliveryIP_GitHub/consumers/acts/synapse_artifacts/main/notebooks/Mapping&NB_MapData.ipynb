{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import json\r\n",
        "import pyspark.sql.functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "#PARAMETERS\r\n",
        "raw_folderpath = \"\"\r\n",
        "raw_filename = \"\"\r\n",
        "mappingJSON = \"\"\r\n",
        "sinkdbTableName = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Storage Config\r\n",
        "storageLinkedService = 'LS_DataLake'\r\n",
        "storageAccount_ls = mssparkutils.credentials.getPropertiesAll(storageLinkedService)\r\n",
        "storage_account = json.loads(storageAccount_ls)['Endpoint'].split('.')[0].replace('https://','')\r\n",
        "container = 'raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define Mapping Function\r\n",
        "def MapData(mappingJSON, container, storage_account, folderpath, filename, table):\r\n",
        "\r\n",
        "    #Step 1: Read File\r\n",
        "    initialDataframe = spark.read.csv( 'abfss://'+container+'@'+ storage_account + '.dfs.core.windows.net/'+ folderpath+'/'+ filename, sep=',',\r\n",
        "                         inferSchema=True, header=True)\r\n",
        "\r\n",
        "    #Step 2: Parse Mapping JSON\r\n",
        "    mapping_json = json.loads(mappingJSON)\r\n",
        "    columnMappingsDict = list(mapping_json['mappings'])\r\n",
        "    columnMappings = [(columnMapping['source'], columnMapping['sink']) for columnMapping in columnMappingsDict]\r\n",
        "\r\n",
        "    #Step 3: Rename Columns\r\n",
        "    updated_columns = [f.col(mapping[0]).alias(mapping[1]) for mapping in columnMappings]\r\n",
        "    newDataframe=initialDataframe.select([c.cast('string') for c in updated_columns])\r\n",
        "\r\n",
        "    return newDataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Map DF\r\n",
        "mapped_df = MapData(mappingJSON, container, storage_account, raw_folderpath, raw_filename, sinkdbTableName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Output to Staging\r\n",
        "mapped_df.write.mode(\"overwrite\").parquet('abfss://'+'staging'+'@'+ storage_account + '.dfs.core.windows.net/Tax/Undefined/sinkdbTableName')"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    }
  }
}